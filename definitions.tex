\subsection{Key Definitions}

In order to make the discussion that follows more precise, we define key terms used in subsequent sections.
We caution that while many of these concepts are familiar, our terminology follows the {\it International Vocabulary of Metrology} (VIM)\citep{JCGM:VIM2012}, a standard that sometimes differs from the conventional or common language of engineering statistics.   For additional information about or clarification of the statistical meaning of terms in the VIM, we suggest that readers consult the {\it Guide to the expression of uncertainty in measurement} (GUM)\citep{JCGM:GUM2008}.

For clarity, we highlight differences between conventional terms and the VIM usage employed throughout this article.
Readers should study the term ``standard uncertainty'' which is sometimes estimated by (in common parlance) the ``standard error of the mean''; however, the VIM term for the latter is the ``experimental standard deviation of the mean.''
In cases of lexical ambiguity, the reader should assume that we hold to the definition of terms as given in the VIM.

The glossary is presented in a logical, rather than alphabetical order, in that each definition requires only those preceding it.  We strongly encourage reading through the full glossary because of its structure and potentially unfamiliar terminology.  Importantly, we also recommend reading the discussion that immediately follows, since this (i) explains the rationale for adopting the chosen language, (ii)  discusses the limited relationship between statistics and uncertainty quantification, and (iii) thereby clarifies our perspective on best-practices. 


\subsubsection{Glossary of Statistical Terms}
\begin{itemize}

\item {\bf Random quantity}: A quantity whose numerical value is inherently unknowable or unpredictable.
  Observations or measurements taken from a molecular simulation are treated as random quantities\footnote{Most molecular simulations (even those using pseudo-random number generators) are deterministic in that the sequence of visited states is generated by a fixed algorithm.  As such, the simulation output is never truly random.
In practice, however, the chaotic nature of the simulation allows for application of the principles of statistics to the analysis of simulation observations.
Thus, observations/measurements taken at points along the simulation may be treated as random quantities.
See Ref.~\cite{Leimkuhler} for more discussion of this rather deep point.}.

%\smallskip
%
%\textbf{\textit{Remark:}}  Informally speaking, we assume that a random quantity can be described in terms of a mean (or expected) value and a dispersion.  The latter characterizes the extent to which a {\it realization} of the random quantity differs from the mean.  More formally, we assume that the probability of a random variable $x$ taking value $x$ is given by $P(x)dx$, where $P(x)$ is a probably density and $dx$ is an infinitesimal width about $x$.

%\smallskip

%\textbf{\textit{Remark:}} Virtually all simulation tools (even those using quasi-random number generators) are deterministic.  As such, the output of any given simulation is never truly random, since it is the composition of knowable and predictable calculations.  However, it is generally infeasible to reproduce simulated calculations by hand, so that these are {\it in practice} unknowable.  Moreover, the chaotic nature of typical simulated systems leads to a situation in which the frequency of a given system configuration is well described by probability densities of statistical mechanics.  See Ref.~\cite{Leimkuhler} for more discussion of this rather deep point.
%Thus, observations/measurements taken at points along the simulation sequence are treated as random quantities to which one can apply the principles of statistics for estimating expectation values and the uncertainty in those estimates.
%\textbf{\textit{Remark:}} Most molecular simulations (even though using pseudo-random number generators) are deterministic in that the sequence of visited states is generated by a fixed algorithm and, as such, the simulation output is never truly random.
%In practice, however, the chaotic nature of the simulation allows for application of the principles of statistics to the analysis of simulation observations.
%Thus, observations/measurements taken at points along the simulation may be treated as random quantities.
%See Ref.~\cite{Leimkuhler} for more discussion of this rather deep point.

\item {\bf True value:}  The value of a quantity that is consistent with its definition and is the objective of an idealized measurement or simulation. The adjective ``true'' is often dropped when reference to the definition is clear by context\citep{JCGM:GUM2008,JCGM:VIM2012}.
  \label{def:true_value}


%\textbf{\textit{Remark:}} Generally speaking, we use the descriptor ``true value'' in reference to quantities such as the expected %value and standard deviation of a simulation output (were we able to run infinitely many simulations).
%As these quantities are inherently unknowable, we can only estimate their values and associated uncertainties.


\item {\bf Standard Uncertainty}: Uncertainty in a result (e.g.\ estimation of a true value) as expressed in terms of a standard deviation\footnote{The definition of standard uncertainty does not specify how to calculate the standard deviation.
    This choice ultimately rests with the modeler and should be dictated by the details of the uncertainty relevant to the problem at hand.}.
  \label{def:std_unc}


\item {\bf Expectation value}:  If $P(x)$ is the probability density of a continuous random quantity $x$, then the expectation value is given by the formula
\begin{align}
  \expval{x} = \int {\rm d}x\, P(x) x.
\end{align}
In the case that $x$ adopts discrete values $x_1,x_2,...$, we instead write
\begin{align}
  \expval{x} = \sum_i x_i P(x_i).
\end{align}

\item {\bf Variance}:\footnote{The true probability density $P(x)$ is inherently unknowable, given that we can only collect a finite amount of data about $x$.  As such, we can only estimate the expectation value of $x$ and its variance.} Taking $P(x)$ as defined previously, the variance of a random quantity is a measure of how much it can fluctuate, given by the formula
\begin{align}
\sigma_x^2 = \int {\rm d}x\, P(x) \left( x  - \expval{x} \right)^2.
\end{align}
If $x$ assume discrete values, the coresponding definition becomes
\begin{align}
\sigma_x^2 = \sum_i P(x_i) \left( x_i  - \expval{x} \right)^2.
\end{align}
{\bf Remark:} The standard deviation is the square root of the variance.

\item {\bf Arithmetic mean}: An estimate of the (true) expectation value of a random quantity, given by the formula
  %
  \begin{equation}
    \mean{x} = \dfrac{1}{n} \sum_{k=1}^{n} x_k \label{def:arith_mean}
  \end{equation}
  %
  where $x_j$ is an experimental or simulated realization of the random variable and $n$ is the number of samples. 
\smallskip 

\textbf{\textit{Remark:}} This quantity is often called the ``sample mean.''
Note that a proper realization of a random variable (with no systematic bias) will yield values distributed according to $P$, so $\mean{x} \rightarrow \expval{x}$ as $n \rightarrow \infty$.

\item {\bf Experimental standard deviation\footnote{
      The term ``experimental'' can refer to simulated data, since these are the results of numerical experiments.
}
}: An estimate of the (true) standard deviation of a random variable, given by the formula\footnote{The factor of $n-1$ (as opposed to $n$) appearing in the demonimator of Eq.~\ref{def:exp_st_dev} is needed to ensure that the estimate is {\it unbiased}, meaning that on average $\stdev{x_k}^2$ is equal to the true variance. Physically, we can interpret the $-1$ as accounting for the fact that one degree-of-freedom (e.g.\ piece of data) is lost via the appearance of $\mean{x}$ in the definition of $\stdev{x_k}$.  Equivalently, it accounts for the fact that the arithmetic mean is linearly correlated with each $x_k$ (cf.\ \hyperref[def:unc_obs]{Linearly Uncorrelated Observables}).}
  \begin{equation}
    \stdev{x_k} = \sqrt{\dfrac{\sum_{j=1}^n\left(x_j - \mean{x}\right)^2}{n-1}} \label{def:exp_st_dev}
  \end{equation}
  \smallskip
  \textbf{\textit{Remark:}} This quantity is often called the ``sample standard deviation.'' 
%FYI, the data does NOT need to be linearly uncorrelated for this estimate under typical MD conditions.  Assuming correlation times \tau << N (where N is the number of timesteps), the relative error in using correlated data is O(tau/N) << 1.


  \item {\bf Linearly Uncorrelated observables}:  If quantities $x_j$ and $x_k$ have mean values $\expval{x_j}$ and $\expval{x_k}$, then $x_j$ and $x_k$ are linearly uncorrelated if
\begin{equation}
  \expval{ \left(x_j - \expval{x_j} \right) \left(x_k - \expval{x_k} \right) }r=0 \label{def:unc_obs}
\end{equation}
where $\expval{\star}$ denotes the (true) expectation value.

\smallskip

\textbf{\textit{Remark:}} Linear uncorrelation and independence of random variables are concepts that are often conflated.  The latter amounts to the statement that the joint density of two random variables $x$ and $y$ can be decomposed as $P(x,y)=P(x)P(y)$, which is stronger than linear uncorrelation.  Empirically testing for independence, however, is not practical, nor is it necessary for any of the estimates discussed in this work.
  
\item {\bf Experimental standard deviation of the mean}: An estimate of the standard deviation of the distribution of the \hyperref[def:arith_mean]{arithmetic mean}, given by the formula
  % 
  \begin{equation}
    \stdevmean{\mean{x}} = \dfrac{\stdev{x_k}}{\sqrt{n}}, \label{def:exp_st_dev_mean}
  \end{equation}
  where the realizations of $x_k$ are assumed to be linearly uncorrelated.
  
%  \smallskip
  
%\textbf{\textit{Remark:}} The experimental standard deviation of the mean characterizes the dispersion of the arithmetic mean relative to its true value.

\smallskip
\textbf{\textit{Remark:}} This quantity is often called the ``standard error.''  In the section on correlation analyses, we generalize this formula.

%\item {\bf Precision}: The amount of variability in an estimate (e.g.\ based on repeating a given simulation protocol multiple times).  %Better sampling in an individual simulation leads to higher precision.  
  % I commented out the last phrase because ``better'' seems a bit vague to me in this context.  Should this comment be placed elsewhere?
  %PNP I say no to placing this elsewhere.  I agree that ``better'' is too vague.  It could refer to a lot of things, including decreasing the timestep to reduce effects of numerical errors, longer sampling, using a different thermostat, etc.  Also, I would get rid of this definition and the next
% \label{def:precision}

%\item {\bf Accuracy}: The degree to which a result agrees with a reference value.
%  The latter may be an experimental measurement or the result of a well-sampled simulation.
%  \label{def:accuracy}
  
\item {\bf Raw data}: The numbers that the computer program directly generates as it proceeds through a sequence of states.
For example, a MC simulation generates a sequence of configurations, for which there are associated properties such as the instantaneous pressure, temperature, volume, etc.
\label{def:raw_data}
\item {\bf Derived observables}: Quantities derived from `non-trivial' analyses of raw data, e.g.\ properties that may not be computed for a single configuration such as free energies.
  \label{def:deriv_obs}
  
%\item {\bf Independent observables}: Random quantities $x$ and $y$ for which the joint probability density can be written as a %product of individual probability densities $P(x,y)=P(x)P(y)$.
 % \label{def:ind_obs}
  %I would in principle get rid of this definition; it's not actually needed
  

%PNP: If we get rid of independent random variables, we can also get rid of this remark

%\item {\bf Linearly correlated observables}: Random quantities that are linearly correlated.
%  \label{def:corr_obs}

\item {\bf Correlation time}: In time-series data of a random quantity $x(t)$ (e.g.\ a physical property from a MC or MD trajectory), this is the time $\tau$ over which $x(t)$ and $x(t+\tau)$ remain (linearly) correlated\footnote{Generally speaking, MC and MD trajectories generate new configurations from preceding ones.
Thus, the correlation time can be interpreted as the time over which the system retains memory of its previous states.
Such correlations are often {\bf stationary}, meaning that $\tau$ is independent of $t$.
Roughly speaking, the total simulation time divided by the longest correlation time yields an order-of-magnitude estimate of the number of (linearly) {\it uncorrelated} samples generated by a simulation.}.
%DWS NOTE: in actuality, don't we use t_{run}/(2\tau) to estimate the number of uncorrelated samples
%PNP: true, it might be worthwhile putting in the 2.  But admittedly I think we're only after an order-of-magnitude estimate, for which O(2)=O(1).  I changed the text slightly to reflect this.
\label{def:corr_time}

%\smallskip
%
%\textbf{\textit{Remark:}} Generally speaking, MC and MD trajectories generate new configurations from preceding ones.
%Thus, the correlation time can be interpreted as the time over which the system retains memory of its previous states.
%%Such correlations are often {\bf stationary}, meaning that $\tau$ is independent of $t$.
%Roughly speaking, the total simulation time divided by the longest correlation time yields an estimate of the number of {\it %uncorrelated} samples generated by a simulation.

%The time over which realizations of a random quantity indexed by a time-series (e.g.\ a physical property from a Monte Carlo or molecular dynamics trajectory) retain ``memory'' of one another, since each configuration is generated from the preceding one. Roughly, the total simulation time divided by the (longest) correlation time gives an estimate of the number of \emph{independent} samples that will govern overall statistical quality of the data.

\item {\bf Two-sided confidence interval}: A statistically derived pair of lower and upper bounds between which the (true) expectation value of a random quantity is likely to fall, as quantified by a {\it confidence level} (typically given as a percentage, e.g.\ 95~\%).
  For Gaussian-distributed data, the confidence level, in conjunction with the sample size, determines the {\it coverage factor}, which is multiplied by \hyperref[def:exp_st_dev_mean]{$\stdevmean{\mean{x}}$} to assign the bounds of the confidence interval.
  \label{def:conf_int}

\end{itemize}

% DWS NOTE: Let's think about which terms we need to define. Basically, if a term is not used in the paper, then we almost certainly do not need to include it in the glossary. For instance, do we need to include: "independent observables" or "uncorrelated observables"; similarly for "precision" and "accuracy"?

%PNP Reply: I agree that we don't need terms that aren't used, such as precision and accuracy.  I have a slight preference for getting rid of those.  Independent and uncorrelated are trickier.  Some definitions reference those concepts, although they may not appear directly in the actual discussion that follows.  For example, the notion of a correlation time requires that we know what an uncorrelated RV is, or at least it invokes that concept.  Moreover, the basic uncertainty estimates we invoke assume uncorrelation (but not independence).  So I think the definition of uncorrelated RV needs to stay.  Independent RV is trickier.  I personally dislike introducing that idea in this text, and that was the source of some back-and-forth between Dan Z. and me.  Basically, we can never test that two RVs are independent, only that they are linearly uncorrelated.  In other words, independence is really a mathematician's concept, whereas uncorrelated in an empirical concept that modelers could actually run into.  I prefer to not waste our reader's brain-cell cycles on the former.



\subsubsection{Terminology and its relation to our broader perspective on uncertainty}

As surveyed by Refs.~\citep{JCGM:GUM2008,JCGM:VIM2012}, the discussion that originally motivated many of theses definitions appears rather philosophical.  However, there are practical issues at stake related to both the content of the definitions as well as the need to adopt their usage.  We review such issues now.
%Nonetheless, it bears repeating here, if only to force a level of honesty regarding what we can actually hope to achieve with simulations.

At the heart of the matter is the observation that any uncertainty analysis, no matter how thorough, is inherently subjective.  This can can be understood, for example, by noting that the arithmetic mean is itself actually a random quantity that only approximates the true expectation value.\footnote{Notably, the same observation applies to the experimental standard deviation and the corresponding experimental standard deviation of the mean.}  Because its variation relative to the true value depends on the number of samples (notwithstanding a little bad luck), one could therefore argue that a better mean is always obtained by collecting more data.   We cannot collect data indefinitely, however, so the {\it quality} of an estimate necessarily depends on a {\it choice} of when to stop.  Ultimately, this discussion forces us to acknowledge that {\it the role of any uncertainty estimate is to facilitate decision-making,} and, as such, the thorougness of any analysis should be tailored to the decision at hand.  

Practically speaking, the definitions as put forth by the VIM attempt to reflect this perspective while also capturing ideas that the statistics community have long found useful.  For example, the concept of an ``experimental standard deviation of the mean'' is nothing more than the ``standard error of the mean.''  However, the adjective ``experimental'' explicitly acknowledges that the estimate is in fact obtained from observation (and not analytical results), while the use of ``deviation'' in place of ``error'' emphasizes that the latter is unknowable.  Similar considerations apply to the term ``experimental standard deviation,'' which is more commonly referred to as the ``sample standard deviation.''

It is important to note that subjectivity as identified in this discussion does not arise just from questions of sampling. In particular, methods such as parametric bootstrap and correlation analyses (discussed below) invoke modeling assumptions that can never be objectively tested.  Moreover, experts may not even agree on how to compute a derived quantity, which leads to ambiguity in what we mean by a ``true value.''\cite{patrone1}  That we should consider these issues carefully and assess their impacts on any prediction is reflected in the definition of the ``standard uncertainty,'' which does not actually tell us how to compute uncertainties.  {\it Rather it is the task of the modeler to consider the impacts of their assumptions and choices when formulating a final uncertainy estimate.  To this end, the language we use plays a large role in how well these considerations are communicated.}

As a final thought, we reiterate that the goal of an uncertainty analysis is not necessarily to perform the most thorough computations possible, but rather to communicate clearly and openly what has been assumed and done.  We cannot predict every use-case for data that we generate, nor can we anticipate the decisions that will be made on the basis of our predictions.  The importance of clearly communicating therefore rests on the fact that in doing so, we allow others to decide for themselves whether our analysis is sufficient or requires revisiting.  To this end, consistent and precise use of language plays an important, if understated role.


%DWS NOTE: I think that this following discussion is entirely accurate, but is so philosophical that it doesn't help the article. Hence my preceding rewrite.

%PNP response: I disagree with some elements of the rewrite.  

%Let me paraphrase my understanding of how you see this; correct me if I'm wrong.  It seems that the two points you highlight are (i) the need to connect the language to the actual task of estimating uncertainties, and (ii) the need to connect VIM language to more common terminology.  I agree with the second task, but I see the first as the goal of the entire manuscript, not just this section.

% In more detail, I feel like the goal of this section (in addition to helping us standardize the manuscript) should be to get readers to think about the words they are using.  At the risk of sounding like a hippie (or something), I think the ultimate hope (which may be unrealistic) is really to induce a mindset change.  Right now I see folks blindly going around computing standard deviations without really thinking about why or what they're trying to express.  In my original draft of this section, I highlighted the definitional uncertainty precisely because (i) folks typically don't think about it, and (ii) it's a type of uncertainty that can lead to order-of-magnitude differences in estimates coming out of simulations.  (My favorite example of this is bilinear-fits versus hyperbola center estimates of bilinear data).  Moreover, I think definitional uncertainty is exactly the type of ``unknowable truth'' that the vocabulary in VIM is trying to get people to think about, or at least recognize and appreciate.  The corresponding discussion of definitional uncertainty was intended to fit in with the theme of "understand the implications of the words."

% So, in this vein, it seems that there are really two related issues at stake in the definition section:
%1) We can never actually compute the true value of any statistical quantity;
%2) VIM adopts a distinct language that is intended to reflect this fact.
% To that end, I think what you've written is correct in pointing out, e.g., that the true and sample means are different.  But I think that this section should shy away from offering guidelines on what to calculate and how, and focus more on what the terminology is actually trying to communicate.  In particular, I can name several cases where I don't think the arithmetic mean should be used, and likewise where the experimental standard deviation of the mean is an inappropriate measure of uncertainty.  I think we will confuse the reader by trying to be so definite in the absence of concrete examples, and later sections try to address some of these questions anyways.

%Rather, I think the emphasis should be on making the conection between points 1 and 2 above.  Admittedly my version was a little philosophical, so it makes sense to compromise on language.  What are your thoughts though?  Do you see this section differently?


%As we recognize that the glossary of statistical terms is overly pedantic, it is valuable to subsequently connect those formal terms to the ultimate goal of this guide, to enable the estimation of uncertainty in thermophysical properties obtained via molecular simulation.
%Additionally, this section will connect the formal VIM definitions to more common usages for accessibility.
%We again reiterate here that the statistical definitions in the VIM\cite{JCGM:VIM2012} are chosen to both minimize ambiguity in the particular word choice and honestly restrict the statistical capability of their associated mathematical quantities.

%Firstly, we point out that a property obtained by analysis of results from a molecular simulation (either directly from raw data or as a derived observable) should be quantified via the \hyperref[def:arith_mean]{arithmetic mean} (i.e., the sample mean, using older terminology).
%This mean property is emphatically {\it not} the \hyperref[def:true_value]{true value}; an infinite number of simulations would be necessary to properly obtain the true value.
%Secondly, the uncertainty in the arithmetic mean should be quantified via the \hyperref[def:exp_st_dev_mean]{experimental standard deviation of the mean} (i.e., the standard error, using older terminology).
%This standard deviation characterizes the dispersion of the arithmetic mean relative to its true value; it is {\it not} the statistical variation in the true value itself (e.g.\ it is not used to compute fluctuation quantities).
%Lastly, results of a molecular simulation should be reported as 1) the arithmetic mean with 2) an appropriate \hyperref[def:conf_int]{confidence interval}, e.g., as $\bar{x} \pm c$, where $\bar{x}$ is the arithmetic mean (of the property of interest) and $c$ is the size of the confidence interval.
%Calculation of the confidence interval, which requires preselection of a confidence level based on field-specific heuristics, is described in Section \ref{sec:specific}.

%While the glossary of statistical terms is (arguably) overly pedantic, the terms are chosen\cite{JCGM:VIM2012} to best describe the connection between their intended usage and the underlying statistical concepts.





%The fundamental idea underpinning this discussion is the observation that true values are inherently unknowable.
%As an added twist, the {\it Uncertainty Approach} advocated by Ref.~\citep{JCGM:GUM2008} also points out that the {\it definitions of true values may be imprecise,} so that the latter are in fact not even uniquely defined.\footnote{The GUM treats this definitional uncertainty as negligible compared to other sources.
%However, the notion of definitional uncertainty is worth revisiting in the context of simulated data.
%For example, we are aware of at least one situation in which the large scale of noise in simulated data (as compared with experiments) induces subjectivity in the definition of the glass-transition temperature, which ultimately affects uncertainties in a non-trivial way.
%See Ref.~\citep{patrone1} and \textcolor{red}{Cite TG doc}.}
%As such, assignments of ``error'' are impossible to make, since these are defined relative to the true value.
%Rather, statistical analyses are better interpreted as quantifying our state of knowledge, e.g.\ our ability to confidently state that the the ``true'' mean is within some interval.

%From a practical standpoint, this leads one to revisit definitions of uncertainty as defined, for example, by Eqs.~\ref{def:exp_st_dev} and \ref{def:exp_st_dev_mean}.
%Conventionally this has gone by the name ``standard error,'' but in keeping with the perspective already laid out, we advocate the use of \hyperref[def:exp_st_dev_mean]{experimental standard deviation of the mean}.
%To maintain consistency with the VIM and GUM, we also use ``sample mean'' and ``sample standard deviation'' with their counterparts ``arithmetic mean'' and ``experimental standard deviation.''
%The reader should be aware that this older language is still used frequently throughout the literature.


%Table of equivalencies

%Arithmetic mean = ``sample mean''\\
%Experimental standard deviation: ``sample standard deviation''\\
%Experimental standard deviation of the mean: ``standard error''\\

