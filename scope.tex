\subsection{Scope}

Simulating molecular systems that are interesting by today's standards -- whether for biomolecular study, materials science, or a related field -- is a challenging task.
More ambitious simulations are inevitably performed every year, but even systems which would be considered simple and far from the cutting edge, such as short alkanes in liquid phase, provide challenges \cite{Schappals2017}.
In addition to the various system-specific issues that modelers must address, questions often arise concerning the best way to adequately sample the desired phase-space or estimate uncertainties.  And while these latter questions are not unique to molecular modeling, their importance cannot be overstated: the usefulness of a simulated result ultimately hinges on being able to confidently and accurately report uncertainties along with any given prediction.  In the context of techniques such as molecular dynamics (MD) and Monte Carlo (MC), these considerations are especially important, given that even large-scale modern computing resources are no guarantee of adequate sampling.

This article therefore aims to provide best-practices for reporting simulated observables, assessing confidence in simulations, and deriving uncertainty estimates (more colloquially, ``error bars'') based on a variety of statistical techniques applicable to physics-based sampling methods and their associated ``enhanced'' counterparts.  As a general rule, we advocate a tiered approach to computational modeling: workflows should begin with back-of-the-envelope calculations to determine the feasibility of a given computation, followed by the actual simulation(s).  Semi-quantitative checks can then be used to check for adequate sampling and assess the quality of data.  Only once these steps have been performed should one actually construct estimates of observables and uncertainties.  In this way, modelers avoid unncessary waste by continuously gauging the likelihood that subsequent steps will be successful.  Moreover, this approach can help to identify seemingly reasonable data that may have little value for prediction and/or the result of a poorly run simulation.

It is worth emphasizing that in the last few years, many works have developed and advocated for uncertainty quantification (UQ) methods not traditionally used in the MD and MC communities.  In some cases, these methods buck trends that have become longstanding conventions in certain communities, e.g.\ the practice of only using uncorrelated data to construct statistical estimates.  A key goal of this manuscript is therefore to advocate newer UQ methods when these are demonstrably better.  Along these lines, we wish to remind the reader that better results are not only obtained from faster computers, but also by using data more thoughtfully.  

Importantly, the reader should be aware that there is not a ``one-size-fits-all'' approach to UQ.  Ultimately, we take the perspective that uncertainty quantification in its broadest sense aims to provide actionable information for the purposes of making decisions, e.g.\ in an industrial R \& D setting or in planning future academic studies.  A simulation protocol and subsequent analysis of its results should therefore take into account the intended audience and/or decisions to be made on the basis of the computation.  In some cases, quick-and-dirty workflows can indeed be useful if the goal is to only provide order-of-magnitude estimates of some quantity.  We also note that uncertainties can often be estimated through a variety of techniques, and there may not be consensus as to which, if any, are best.  {\it Thus, a critical component of any UQ analysis is communication, e.g.\ of the assumptions being made, the tools used, and the way that results are interpreted.}  Educated decisions can only be made through an understanding.  

While UQ is a central topic of this manuscript, our scope is nonetheless limited to issues associated with sampling and related uncertainty estimates.  We do not address systematic errors arising from inaccuracy of force-fields, the underlying model, or parametric choices such as the choice of a thermostat time-constant.  See, for example, Refs.~\cite{Leimkuhler,Rizzi2,Rizzi3,Rizzi4} for methods that address such problems.  Moreover, we do not consider model-form error and related issues that arise when comparing simulated predictions with experiment.  Rather, we take the raw trajectory data at face value, assuming that it is a valid description of the system of interest.\footnote{In more technical UQ language, we restrict our scope to {\it verification} of simulation results, as opposed to {\it validation}.}




%PNP note: originally we had the phrase, "Some problems and systems may be better studied with cruder techniques and analyses, which will not be covered here."  This seems like an attempt to limit the scope, but I don't know what the original author had in mind.  What types of analyses are we ruling out versus keeping in?


%simulation studies attempting to quantify observables and derive reliable estimates of uncertainty (e.g., error bars) based on `standard' canonical sampling methods (e.g., molecular dynamics and Monte Carlo) and associated `enhanced' sampling methods.
%Some problems and systems may be better studied with cruder techniques and analyses, which will not be covered here.
%This article also will not cover issues of systematic error arising from inaccuracy in force field (underlying model) or even from the simulation setup.
%Rather, we will take raw trajectory data at face value, assuming it is a valid outcome given the underlying model.
%We emphatically will \emph{not} assume that a trajectory has been sufficiently `equilibrated' or 'converged'.

%PNP note:  The last sentence above is a bit misleading because we have a whole section on equilibrating.  It think it's better to drop 
